ğŸš€ Gemini RAG DocChat

A full end-to-end Retrieval-Augmented Generation (RAG) chatbot using FastAPI, Qdrant, Gemini API, and React.

Live now: https://gemini-rag-docchat.vercel.app/

â¸»

ğŸ“Œ Overview

Gemini RAG DocChat is a document-aware AI assistant.
Users can upload PDFs or text files, the system extracts & chunks text, stores semantic embeddings in Qdrant, and answers questions grounded in the uploaded content using Google Gemini models.

This project was built end-to-end from scratch to learn how modern RAG systems work under the hood â€” covering ingestion, chunking, embeddings, vector search, retrieval fusion, and grounded LLM generation.

âœ¨ Live App: (Frontend on Vercel, Backend on Render, Qdrant Cloud)
ğŸ“¦ Frontend: React + Vite
ğŸ§  Backend: FastAPI
ğŸ“Š Vector DB: Qdrant Cloud
ğŸ¤– LLM: Gemini 1.5 Flash + text-embedding-004

â¸»

ğŸ¯ Features

ğŸ”¼ Upload & Index
	â€¢	Upload multiple files (.pdf, .txt, .md)
	â€¢	Stored by namespace (per user/session grouping)
	â€¢	Automatic text extraction + cleanup
	â€¢	Document chunking (600 words with 80 overlap)
	â€¢	Embeddings from Gemini text-embedding-004
	â€¢	Stored in Qdrant with metadata: filename, page, namespace

ğŸ” Hybrid Retrieval
	â€¢	Dense semantic search (vector similarity)
	â€¢	BM25 keyword retrieval
	â€¢	Fused scoring: score = Î±*dense + (1-Î±)*bm25
	â€¢	Configurable top_k and alpha

ğŸ¤– Grounded Chat
	â€¢	Detects small talk vs document questions
	â€¢	Includes contextual snippets in prompt
	â€¢	Strict grounded prompt: â€œUse only the provided snippetsâ€
	â€¢	Returns answer + clean citation list

ğŸŒ Deployment
	â€¢	Frontend: Vercel
	â€¢	Backend: Render (FastAPI + Uvicorn)
	â€¢	Vector DB: Qdrant Cloud
	â€¢	Fully CORS-safe and cross-domain JSON API

â¸»

ğŸ—ï¸ Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        React Frontend         â”‚
â”‚  (Upload UI + Chat Interface) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        FastAPI Backend         â”‚
â”‚ /upload  /index  /ask  /search â”‚
â”‚                                â”‚
â”‚ 1. Extract & Chunk             â”‚
â”‚ 2. Generate Embeddings         â”‚
â”‚ 3. Hybrid Retrieval (dense+bm25)â”‚
â”‚ 4. Grounded Gemini Answers     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚    Qdrant Cloud    â”‚
      â”‚ (Vector + Payload) â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¸»

ğŸ§ª Evaluation

A small evaluation of 20â€“30 questions showed:

Metric	Result
Answer Accuracy	~80%
Faithfulness	~90%
Citation Quality	~75%
Hybrid Retrieval Benefit	+15% recall
Small Talk Quality	100%


â¸»

ğŸ“‚ Project Structure

gemini-rag-docchat/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ routes/          # upload, index, ask, search
â”‚   â”‚   â”œâ”€â”€ services/        # ingest, chunking, indexing
â”‚   â”‚   â”œâ”€â”€ ai/              # embeddings, prompts, generator
â”‚   â”‚   â”œâ”€â”€ db/              # qdrant client setup
â”‚   â”‚   â”œâ”€â”€ retriever/       # dense + bm25 fusion
â”‚   â”‚   â”œâ”€â”€ state/           # short chat memory
â”‚   â”‚   â”œâ”€â”€ models/          # chunk metadata
â”‚   â”‚   â””â”€â”€ main.py          # FastAPI bootstrap
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api.js           # backend calls
â”‚   â”‚   â”œâ”€â”€ views/           # Upload + Chat pages
â”‚   â”‚   â”œâ”€â”€ components/      # UI components
â”‚   â””â”€â”€ vite config
â”‚
â””â”€â”€ README.md


â¸»

ğŸ› ï¸ Local Development

1. Clone the repo

git clone https://github.com/<yourname>/gemini-rag-docchat
cd gemini-rag-docchat


â¸»

2. Backend setup

cd backend
pip install -r requirements.txt

Create .env inside backend/:

GOOGLE_API_KEY=<your_gemini_key>
QDRANT_URL=https://<cluster>.<region>.cloud.qdrant.io:6333
QDRANT_API_KEY=<your_qdrant_key>
COLLECTION_NAME=docs
EMBED_DIM=768
CORS_ORIGINS=http://localhost:5173

Run:

uvicorn app.main:app --reload


â¸»

3. Frontend setup

cd ../frontend
npm install
npm run dev

The app runs on:
	â€¢	Backend â†’ http://127.0.0.1:8000
	â€¢	Frontend â†’ http://127.0.0.1:5173

â¸»

â˜ï¸ Deployment

â–¶ï¸ Backend â€“ Render
	â€¢	Connect repo to Render Web Service
	â€¢	Root Directory â†’ backend/
	â€¢	Build: pip install -r requirements.txt
	â€¢	Start: uvicorn app.main:app --host 0.0.0.0 --port 8000
	â€¢	Add environment variables:

GOOGLE_API_KEY
QDRANT_URL
QDRANT_API_KEY
COLLECTION_NAME=docs
EMBED_DIM=768
CORS_ORIGINS=https://<your-vercel-app>.vercel.app



â–¶ï¸ Vector DB â€“ Qdrant Cloud
	â€¢	Create HTTPS cluster
	â€¢	Add collection docs (size 768, cosine)
	â€¢	Use API key above

â–¶ï¸ Frontend â€“ Vercel
	â€¢	Root Directory: frontend/
	â€¢	Build Command: npm run build
	â€¢	Output Directory: dist
	â€¢	Update frontend/src/api.js:

export const BASE = "https://<your-backend>.onrender.com";


â¸»

ğŸ§  What I Learned (key takeaways)
	â€¢	How chunking affects retrieval quality
	â€¢	Why hybrid semantic + keyword search boosts accuracy
	â€¢	How to structure vector payloads for citations
	â€¢	Practical grounding techniques to eliminate hallucination
	â€¢	Deploying multi-service fullstack apps (Render + Vercel + Qdrant Cloud)

â¸»

ğŸ“ Future Improvements
	â€¢	Heading-aware chunking
	â€¢	Reranker model for higher precision
	â€¢	Real-time progress for indexing
	â€¢	Unified domain + reverse proxy
	â€¢	Better UI and file history per namespace

â¸»

â­ Acknowledgements
	â€¢	Google Gemini API
	â€¢	Qdrant Vector Database
	â€¢	FastAPI
	â€¢	BM25Okapi (rank-bm25)
	â€¢	React + Vite
